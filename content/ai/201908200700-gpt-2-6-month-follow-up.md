---
title: "GPT-2: 6-month follow-up"
source: "Unknown"
url: "https://openai.com/index/gpt-2-6-month-follow-up"
published: "2019-08-20T07:00:00.000Z"
category: "ai"
summary: "We’re releasing the 774 million parameter GPT-2 language model after the release of our small 124M model in February, staged release of our medium 355M model in May, and subsequent research with partners and the AI community into the model’s potential for misuse and societal benefit. We’re also releasing an open-source legal agreement to make it easier for organizations to initiate model-sharing partnerships with each other, and are publishing a technical report about our experience in coordinating with the wider AI research community on publication norms."
---
Research from these partners will factor into our future release decisions, as will observing how the 774M model is used, and discussing language models with researchers and policymakers to understand the considerations around larger models. As part of our staged release strategy, our current plan is to release the 1558M parameter model in a few months, but it’s plausible that findings from a partner, or malicious usage of our 774M model, could change this.

We think that a combination of staged release and partnership-based model sharing is likely to be a key foundation of responsible publication in AI, particularly in the context of powerful generative models. The issues inherent to large models are going to grow, rather than diminish, over time. We hope that our work on GPT‑2, discussed further in the [technical report⁠(opens in a new window)](https://cdn.openai.com/GPT_2_August_Report.pdf) we’re publishing, will help provide evidence the AI community can draw on when thinking about the publication challenges inherent to some parts of AI research.

-   [GPT](https://openai.com/research/index/?tags=gpt)
-   [Community & Collaboration](https://openai.com/research/index/?tags=community-collaboration)
-   [Reasonings & Policy](https://openai.com/research/index/?tags=reasoning-policy)
-   [Ethics & Safety](https://openai.com/research/index/?tags=ethics-safety)

## Footnotes

1.  A
    
    Having these conversations is difficult, as it involves talking candidly about proprietary systems and it’s unclear who to reach out to in specific organizations to discuss such models and what the appropriate processes are for inter-org discussion about unreleased research.
    
2.  B
    
    These samples were generated via a “human-in-the-loop” process meant to simulate contemporary disinformation operations, where a human generated samples and periodically selected some for exposure to people.
