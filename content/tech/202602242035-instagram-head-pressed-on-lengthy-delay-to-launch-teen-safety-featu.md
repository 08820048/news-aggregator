---
title: "Instagram head pressed on lengthy delay to launch teen safety features, like a nudity filter, court filing reveals"
source: "Sarah Perez"
url: "https://techcrunch.com/2026/02/24/instagram-head-pressed-on-lengthy-delay-to-launch-teen-safety-features-like-a-nudity-filter-court-filing-reveals/"
published: "2026-02-24T20:35:47.000Z"
category: "tech"
summary: "An email chain with Instagram head Adam Mosseri indicated the company was aware of teen safety issues in DMs in 2018, but didn't launch its unwanted nudity filter until 2024."
---
Prosecutors in a lawsuit focused on whether or not social media apps, like Instagram, are addictive and harmful, wanted to know why it took so long for Meta to roll out basic safety tools, like a nudity filter for private messages sent to teens. In April 2024, Meta [introduced](https://techcrunch.com/2024/04/11/meta-will-auto-blur-nudity-in-instagram-dms-in-latest-teen-safety-step/) a feature that would automatically blur explicit images in Instagram DMs — something the company reportedly understood to be an issue nearly six years prior.

In a newly unsealed deposition in a federal lawsuit, Instagram head Adam Mosseri was asked about an August 2018 email chain with Meta VP and Chief Information Security Officer, [Guy Rosen](https://www.linkedin.com/in/guyro/), where he mentioned that “horrible” things could happen via Instagram private messages, also known as DMs. Those horrible things could include dick pics, the plaintiff’s lawyer said, and Mosseri agreed.

However, the Meta exec pushed back at the line of questioning that suggested the company should have informed parents that its messaging system wasn’t monitored, beyond removing CSAM (Child Sexual Abuse Material).

“I think that it’s pretty clear that you can message problematic content in any messaging app, whether it’s Instagram or otherwise,” Mosseri said. He said the company tried to balance people’s interest in privacy with its own interests in safety.

The testimony also revealed new stats about harmful activity on Instagram, revealing that 19.2% of survey respondents, ages 13 to 15, said they had seen nudity or sexual images on Instagram that they didn’t want to see. In addition, 8.4% of 13 to 15-year-olds said they had seen someone harm themselves or threaten to do so on Instagram over the past seven days they used the app.

While a nudity filter is only one of several updates that have been added to Instagram in recent years to protect teens, prosecutors were more interested in its delay to act, rather than whether the app is safer for teens now.

Mosseri was also questioned on other topics, like an email from a Facebook intern in 2017, who said that he wanted to find “addicted” Facebook users and figure out if there were ways to help them.

Techcrunch event

Boston, MA | June 9, 2026

The 2018 email chain was meant to serve as one example that Meta was aware of the risks to minors, but it took the company until 2024 to release a product that addressed the problem of sexual images sent to teens. This includes those images sent by adults who may have engaged in grooming, a process in which an adult builds trust with a minor over time to manipulate or sexually exploit them.

Reached for comment, Meta spokesperson Liza Crenshaw pointed to the other ways the company has worked to keep teens safe over the years, noting that, “for over a decade, we’ve listened to parents, worked with experts and law enforcement, and conducted in-depth research to understand the issues that matter most. We use these insights to make meaningful changes—like introducing Teen Accounts with built-in protections and providing parents with tools to manage their teens’ experiences. We’re proud of the progress we’ve made, and we’re always working to do better,” she said.

The deposition provided by Mosseri took place during one of what are now [several lawsuits](https://www.npr.org/2026/02/18/nx-s1-5716229/zuckerberg-social-media-addiction-trial) looking to hold big tech accountable for harming teens. This [particular case](https://cand.uscourts.gov/cases-e-filing/cases/422-md-03047-ygr/re-social-media-adolescent-addictionpersonal-injury-products), taking place in the U.S. District Court in the Northern District of California, involves plaintiffs alleging that social media platforms are defective because they’re designed to maximize screen time, which encourages addictive behavior in teens. The defendants include Meta, Snap, TikTok, and YouTube (Google).

Similar lawsuits are also underway [in the Los Angeles County Superior Court](https://techcrunch.com/2026/02/17/metas-own-research-found-parental-supervision-doesnt-really-help-curb-teens-compulsive-social-media-use/) and [in New Mexico](https://apnews.com/article/meta-new-mexico-child-exploitation-trial-19195fc680dba782fb971d68082e11a4).

Lawyers across the cases are hoping to prove that the big tech companies prioritized the need for user growth and increased engagement over the potential harms impacting their youngest users.

The timing of these trials comes amid a growing number of laws restricting social media teen use, both in several U.S. states and abroad.

_Updated after publication with Meta’s comment._

Sarah has worked as a reporter for TechCrunch since August 2011. She joined the company after having previously spent over three years at ReadWriteWeb. Prior to her work as a reporter, Sarah worked in I.T. across a number of industries, including banking, retail and software.

You can contact or verify outreach from Sarah by emailing [sarahp@techcrunch.com](mailto:sarahp@techcrunch.com) or via encrypted message at sarahperez.01 on Signal.

[View Bio](https://techcrunch.com/author/sarah-perez/)
