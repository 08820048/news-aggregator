---
title: "Ex-Googlers are building infrastructure to help companies understand their video data"
source: "Kate Park"
url: "https://techcrunch.com/2026/02/09/ex-googlers-are-building-infrastructure-to-help-companies-understand-their-video-data/"
published: "2026-02-09T17:00:00.000Z"
category: "tech"
summary: "Founded by former Google Japan leaders, InfiniMind is building enterprise AI to turn vast, unused video archives into searchable, actionable business intelligence."
---
Businesses are generating more video than ever. From years of broadcast archives to thousands of store cameras and countless hours of production footage, most of it just sits unused on servers, unwatched and unanalyzed. This is dark data: a massive, untapped resource that companies collect automatically but almost never use in a meaningful way. To tackle the problem, Aza Kai (CEO) and Hiraku Yanagita (COO), two former Googlers who spent nearly a decade working together at Google Japan, decided to build their own solution. The duo co-founded InfiniMind, a Tokyo-based startup developing infrastructure that converts petabytes of unviewed video and audio into structured, queryable business data. “My co-founder, who spent a decade leading brand and data solutions at Google Japan, and I saw this inflection point coming while we were still at Google,” Kai said. By 2024, the technology had matured, and the market demand had become clear enough that the co-founders felt compelled to build the company themselves, he added. Kai, who previously worked at Google Japan across cloud, machine learning, ad systems, and video recommendation models and later led data science teams, explained that current solutions force a tradeoff. Earlier approaches could label objects in individual frames, but they couldn’t track narratives, understand causality, or answer complex questions about video content. For clients with decades of broadcast archives and petabytes of footage, even basic questions about their content often went unanswered. What really changed was the progress in vision-language models between 2021 and 2023. That’s when video AI started moving beyond simple object tagging, Kai noted. Falling GPU costs and annual performance gains of roughly 15–20% over the last decade helped, but the bigger story was capability until recently, models just couldn’t do the job, he told TechCrunch. InfiniMind recently secured $5.8 million in seed funding, led by UTEC and joined by CX2, Headline Asia, Chiba Dojo, and an AI researcher at a16z Scout. The company is relocating its headquarters to the U.S., while it continues to operate an office in Japan. Japan provided the perfect testbed: strong hardware, talented engineers, and a supportive startup ecosystem, allowing the team to fine-tune its technology with demanding customers before going global. Its first product, TV Pulse, launched in Japan in April 2025. The AI-powered platform analyzes television content in real time, helping media and retail companies “track product exposure, brand presence, customer sentiment, and PR impact,” per the startup. After pilot programs with major broadcasters and agencies, it already has paying customers, including wholesalers and media companies. Techcrunch event Boston, MA | June 23, 2026 Now, InfiniMind is ready for the international market. Its flagship product, DeepFrame, a long-form video intelligence platform capable of processing 200 hours of footage to pinpoint specific scenes, speakers, or events, is scheduled for a beta release in March, followed by a full launch in April 2026, Kai said. image credits: infinimind The video analysis space is highly fragmented. Companies such as TwelveLabs provide general-purpose video understanding APIs for a broad range of users, including consumers, prosumers, and enterprises, Kai said, while InfiniMind focuses specifically on enterprise use cases, including monitoring, safety, security, and analyzing video content for deeper insights. “Our solution requires no code; clients bring their data, and our system processes it, providing actionable insights,” Kai said. “We also integrate audio, sound, and speech understanding, not just visuals. Our system can handle unlimited video length, and cost efficiency is a major differentiator. Most existing solutions prioritize accuracy or specific use cases but don’t solve cost challenges.” The seed funding will help the team continue developing the DeepFrame model, expand engineering infrastructure, hire more engineers, and reach additional customers across Japan and the U.S. “This is an exciting space, one of the paths toward AGI,” Kai said. “Understanding general video intelligence is about understanding reality. Industrial applications are important, but our ultimate goal is to push the boundaries of technology to better understand reality and help humans make better decisions.”
