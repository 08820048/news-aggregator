---
title: "Anthropic vs. the Pentagon: What’s actually at stake?"
source: "Rebecca Bellan"
url: "https://techcrunch.com/2026/02/27/anthropic-vs-the-pentagon-whats-actually-at-stake/"
published: "2026-02-27T19:11:04.000Z"
category: "tech"
summary: "Anthropic and the Pentagon are clashing over AI use in autonomous weapons and surveillance, raising high-stakes questions about national security, corporate control, and who sets the rules for military AI."
---
The past two weeks have been defined by a [clash](https://techcrunch.com/2026/02/23/defense-secretary-summons-anthropics-amodei-over-military-use-of-claude/) between Anthropic CEO Dario Amodei and Defense Secretary Pete Hegseth as the two battle over the military’s use of AI. 

Anthropic refuses to allow its AI models to be used for mass surveillance of Americans or for fully autonomous weapons that conduct strikes without human input. At the same time, Secretary Hegseth has argued the Department of Defense shouldn’t be limited by the rules of a vendor, arguing any “lawful use” of the technology should be permitted.

On Thursday, [Amodei publicly signaled](https://techcrunch.com/2026/02/26/anthropic-ceo-stands-firm-as-pentagon-deadline-looms/) that Anthropic isn’t backing down — despite threats that his company could be designated as a supply chain risk as a result. But with the news cycle moving fast, it’s worth revisiting exactly what’s at stake in the fight.

At its core, this fight is about who controls powerful AI systems — the companies that build them, or the government that wants to deploy them.

## What is Anthropic worried about?

As we said above, Anthropic doesn’t want its AI models to be used for mass surveillance of Americans or for autonomous weapons with no humans in the loop for targeting and firing decisions. Traditional defense contractors typically have little say in how their products will be used, but Anthropic has argued from its inception that AI technology poses unique risks and therefore requires unique safeguards. From the company’s perspective, the question is how to maintain those safeguards when the technology is being used by the military.

The U.S. military already relies on highly automated systems, some of which are lethal. The decision to use lethal force has historically been left to humans, but there are few legal restrictions on military use of autonomous weapons. The DoD doesn’t categorically ban fully autonomous weapons systems. According to a [2023 DOD directive](https://www.esd.whs.mil/portals/54/documents/dd/issuances/dodd/300009p.pdf?utm_source=chatgpt.com), AI systems can select and engage targets without human intervention, as long as they meet certain standards and pass review by senior defense officials.

That’s precisely what makes Anthropic nervous. Military technology is secretive by nature, so if the U.S. military were taking steps to automate lethal decision-making, we might not know about it until it was operational. And if it used Anthropic’s models, it could count as “lawful use.”

Techcrunch event

Boston, MA | June 9, 2026

Anthropic’s position isn’t that such uses should be permanently off the table. It’s that its models aren’t capable enough to support them safely yet. Imagine an autonomous system misidentifying a target, escalating a conflict without human authorization, or making a split-second lethal decision that no one can reverse. Put a less-capable AI in charge of weapons, and you get a very fast, very confident machine that’s bad at making high-stakes calls.

AI also has the power to supercharge lawful surveillance of American citizens to a concerning degree. Under current U.S. laws, surveillance of American citizens is already possible, whether through collection of texts, emails, and other communication. AI changes the equation by enabling automated large-scale pattern detection, entity resolution across datasets, predictive risk scoring, and continuous behavioral analysis.

## What does the Pentagon want?

The Pentagon’s argument is that it should be able to deploy Anthropic’s technology for any lawful use it deems necessary, rather than be limited by Anthropic’s internal policies on things like autonomous weapons or surveillance. 

More specifically, Secretary Hegseth has argued the Department of Defense shouldn’t be limited by the rules of a vendor and that it would engage in “lawful use” of the technology.

Sean Parnell, the Pentagon’s chief spokesperson, said in a [Thursday X post](https://x.com/SeanParnellASW/status/2027072228777734474) that the department has no interest in conducting mass domestic surveillance or deploying autonomous weapons. 

“Here’s what we’re asking: Allow the Pentagon to use Anthropic’s model for all lawful purposes,” Parnell said. “This is a simple, common-sense request that will prevent Anthropic from jeopardizing critical military operations and potentially putting our warfighters at risk. We will not let ANY company dictate the terms regarding how we make operational decisions.”

He added that Anthropic has until 5:01 p.m. ET on Friday to decide. “Otherwise, we will terminate our partnership with Anthropic and deem them a supply chain risk for DOW,” he said.

Despite the DoD’s stance that it simply doesn’t believe it should be limited by a corporation’s usage policies, Secretary Hegseth’s concerns about Anthropic have at times seemed connected to cultural grievance. In [a speech at SpaceX and xAI offices in January](https://www.war.gov/News/Transcripts/Transcript/Article/4377190/remarks-by-secretary-of-war-pete-hegseth-at-spacex/), Hegseth railed against “woke AI” in a speech that some saw as a preview of his feud with Anthropic.

“Department of War AI will not be woke,” Hegseth said. “We’re building war-ready weapons and systems, not chatbots for an Ivy League faculty lounge.”

## So what now?

The Pentagon has threatened to either declare Anthropic a “supply chain risk” — which effectively blacklists Anthropic from doing business with the government — or invoke the Defense Production Act (DPA) to force the company to tailor its model to the military’s needs. Hegseth has given Anthropic until 5:01 p.m. on Friday to respond. But with the deadline approaching, it’s anyone’s guess whether the Pentagon will make good on its threat.

This is not a fight either party can easily walk away from. Sachin Seth, a VC at Trousdale Ventures who focuses on defense tech, says a supply chain risk label for Anthropic could mean “lights out” for the company. 

However, he said, if Anthropic is dropped from the DoD, it could be a national security issue.

“\[The Department\] would have to wait six to 12 months for either OpenAI or xAI to catch up,” Seth told TechCrunch. “That leaves a window of up to a year where they might be working from not the best model, but the second or third best.”

xAI is gearing up to become classified-ready and replace Anthropic, and it’s fair to say given owner [Elon Musk’s rhetoric](https://x.com/elonmusk/status/2027294561467613256) on the matter that the company would have no problem giving the DoD total control over its technology. Recent [reports](https://x.com/Hadas_Gold/status/2027389332445671498?s=20) indicate that OpenAI may stick to the same red lines as Anthropic.
