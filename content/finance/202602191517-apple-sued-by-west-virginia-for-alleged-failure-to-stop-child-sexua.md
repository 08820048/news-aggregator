---
title: "Apple sued by West Virginia for alleged failure to stop child sexual abuse material on iCloud, iOS devices"
source: "Unknown"
url: "https://www.cnbc.com/2026/02/19/apple-sued-csam-icloud-ios.html"
published: "2026-02-19T15:17:37.000Z"
category: "finance"
summary: "The lawsuit accuses Apple of prioritizing privacy branding and its own business interests over child safety."
---
![West Virginia AG sues Apple over alleged failure to stop child sexual abuse material on iCloud](https://image.cnbcfm.com/api/v1/image/108267350-17715137281771513726-44110274539-1080pnbcnews.jpg?v=1771513727&w=750&h=422&vtcrop=y)

watch now

West Virginia's attorney general has filed a consumer protection lawsuit against [Apple](https://www.cnbc.com/quotes/AAPL/), alleging that it has failed to prevent child sexual abuse materials from being stored and shared via iOS devices and iCloud services.

John "JB" McCuskey, a Republican, accused Apple of prioritizing privacy branding and its own business interests over child safety, while other big tech companies, including [Google](https://www.cnbc.com/quotes/GOOGL/), [Microsoft](https://www.cnbc.com/quotes/MSFT/), and [Dropbox](https://www.cnbc.com/quotes/DBX/), have been more proactive, using systems like PhotoDNA to combat such material.

PhotoDNA, developed by Microsoft and Dartmouth College in 2009, uses "[hashing and matching](https://www.thorn.org/blog/photodna-leads-fight-against-child-sex-abuse-imagery/)" to automatically identify and block child sexual abuse material (CSAM) images when they have already been identified as such and reported to authorities.

In 2021, [Apple](https://www.cnbc.com/quotes/AAPL/) had tested its own CSAM-detection features that could automatically find and remove images of child exploitation, and report those that had been uploaded to iCloud in the U.S. to the National Center for Missing & Exploited Children.

But the company withdrew its plans for the features after privacy advocates who worried that this technology could create a back door for government surveillance, and be tweaked and exploited to censor other kinds of content on iOS devices.

The company's efforts since then have not satisfied a broad array of critics.

## Read more CNBC tech news

-   [Amid Epstein fallout, Bill Gates becomes point of controversy at India AI summit](https://www.cnbc.com/2026/02/19/amid-epstein-fallout-bill-gates-becomes-point-of-controversy-at-india-ai-summit-.html)
-   [OpenAI and Anthropic's rivalry on display as CEOs avoid holding hands at AI summit](https://www.cnbc.com/2026/02/19/openai-sam-altman-anthropic-dario-amodei-india-ai-summit.html)
-   [Chinese tech companies progress 'remarkable,' OpenAI's Altman tells CNBC](https://www.cnbc.com/2026/02/19/openai-sam-altman-india-ai-summit.html)
-   [Anthropic is clashing with the Pentagon over AI use. Here's what each side wants](https://www.cnbc.com/2026/02/18/anthropic-pentagon-ai-defense-war-surveillance.html)

In 2024, UK-based watchdog the National Society for the Prevention of Cruelty to Children [said Apple](https://www.theguardian.com/technology/article/2024/jul/22/apple-security-child-sexual-images-accusation) failed to adequately monitor, tabulate and report CSAM in its products to authorities.

And in a 2024 lawsuit filed in California's Northern District, thousands of child sexual abuse survivors [sued Apple](https://www.nytimes.com/2024/12/08/technology/apple-child-sexual-abuse-material-lawsuit.html), alleging the company never should have abandoned its earlier plans for CSAM detection features, and by allowing such material to proliferate online, it had caused survivors to relive their trauma.

Apple has positioned itself as the most privacy-sensitive of the Big Tech companies, since its CEO Tim Cook wrote an open letter on the topicÂ [in 2014](https://www.cnbc.com/2014/09/18/apples-tim-cook-takes-a-swipe-at-google-facebook.html).

If West Virginia's suit is successful, it could force the company to make design or data security changes. The state is seeking statutory and punitive damages, and injunctive relief requiring Apple to implement effective CSAM detection.

In an e-mailed statement, a spokesperson for Apple told CNBC that "protecting the safety and privacy of our users, especially children, is central to what we do."

The company pointed to parental controls and features like Communication Safety which, "automatically intervenes on kids' devices when nudity is detected in Messages, shared Photos, AirDrop and even live FaceTime calls," as an indication of its commitment to provide "safety, security, and privacy" to users.

"We are innovating every day to combat ever-evolving threats and maintain the safest, most trusted platform for kids," the spokesperson added.

_\-- CNBC's Kif Leswing contributed reporting_

![California Attorney General launches investigation into xAI, Grok](https://image.cnbcfm.com/api/v1/image/108252540-17684163971768416395-43496270350-1080pnbcnews.jpg?v=1768416396&w=750&h=422&vtcrop=y)

watch now
